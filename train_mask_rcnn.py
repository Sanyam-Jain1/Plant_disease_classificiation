# -*- coding: utf-8 -*-
"""train_mask_rcnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUiqmZgisMS1lz97svk-weBXS_qwDUDj
"""

from google.colab import drive
drive.mount('/content/drive')

"""#INITIALISATION"""

#
# STEP 1: INSTALL DETECTRON2 AND DEPENDENCIES
#
# This cell will take a few minutes to run. It downloads and installs
# PyTorch and the Detectron2 library from its official repository.
#
# NOTE: After this cell finishes, you may need to restart the runtime.
# Colab will usually prompt you to do this automatically. If not, you can
# go to the "Runtime" menu and select "Restart session". This is crucial
# to make sure the newly installed libraries are loaded correctly.
#
import torch, torchvision
import os

# Check the installed PyTorch and CUDA versions
print(f"PyTorch version: {torch.__version__}")
print(f"Torchvision version: {torchvision.__version__}")
print(f"CUDA is available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")

# Install Detectron2
# We use 'git+' to install the latest version directly from GitHub, which is
# often more up-to-date and compatible than the standard pip package.
print("\nInstalling Detectron2...")
!pip install pyyaml==5.1 --quiet
!pip install 'git+https://github.com/facebookresearch/detectron2.git'
print("‚úÖ Detectron2 installation complete.")
print("\n‚û°Ô∏è Please RESTART your runtime now before proceeding to the next step.")

"""#REGISTERING THE DATASET"""



#
# STEP 2: IMPORT LIBRARIES & REGISTER DATASET
#
# This script handles the necessary imports and prepares your custom dataset
# for use with Detectron2.
#

# --- Imports ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger() # Set up Detectron2's default logger

# Import common Detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor, DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data.datasets import register_coco_instances

# Import other common libraries
import os
import numpy as np
import json
import cv2
import random
from google.colab.patches import cv2_imshow # For displaying images in Colab

print("‚úÖ All libraries imported successfully.")

# --- Dataset Configuration & Registration ---

# 1. Define paths to your data on Google Drive
# This is the full annotation file you prepared earlier.
# IMPORTANT: Make sure you have mounted your Google Drive first.
# try:
#     from google.colab import drive
#     drive.mount('/content/drive')
#     print("‚úÖ Google Drive mounted.")
# except:
#     print("Could not mount Google Drive. Assuming files are local.")

# Path to the single, combined COCO JSON file
# We will split this into train/val sets programmatically.
FULL_JSON_PATH = "/content/drive/MyDrive/Dataset_crop_disease/coco_sanitized.json" # Or your corrected file path

# Path to the root directory where the image folders (Pepper, Potato, etc.) are located
IMAGE_ROOT_DIR = "/content/drive/MyDrive/Dataset_crop_disease"

# Paths for the new split files we will create
TRAIN_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_train.json")
VAL_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_val.json")


# 2. Function to split the dataset
def split_coco_dataset(full_json_path, train_path, val_path, split_ratio=0.9):
    """
    Reads a single COCO JSON file and splits it into two separate
    files for training and validation.
    """
    print(f"\nSplitting dataset from: {full_json_path}")
    with open(full_json_path, 'r') as f:
        data = json.load(f)

    images = data['images']
    annotations = data['annotations']
    categories = data['categories']

    # Shuffle the images to ensure a random split
    random.shuffle(images)
    split_index = int(len(images) * split_ratio)

    train_images = images[:split_index]
    val_images = images[split_index:]

    train_image_ids = {img['id'] for img in train_images}
    val_image_ids = {img['id'] for img in val_images}

    # Filter annotations based on the image split
    train_annotations = [anno for anno in annotations if anno['image_id'] in train_image_ids]
    val_annotations = [anno for anno in annotations if anno['image_id'] in val_image_ids]

    # Create the new JSON data structures
    train_data = {'images': train_images, 'annotations': train_annotations, 'categories': categories}
    val_data = {'images': val_images, 'annotations': val_annotations, 'categories': categories}

    # Write the new files
    with open(train_path, 'w') as f:
        json.dump(train_data, f)
    with open(val_path, 'w') as f:
        json.dump(val_data, f)

    print(f"‚úÖ Split complete.")
    print(f"   - Training set: {len(train_images)} images. Saved to {train_path}")
    print(f"   - Validation set: {len(val_images)} images. Saved to {val_path}")
    return categories

# 3. Perform the split and register the datasets
categories = split_coco_dataset(FULL_JSON_PATH, TRAIN_JSON_PATH, VAL_JSON_PATH)

# Now, register these new JSON files with Detectron2
# The first argument is a unique name for the dataset.
# The last two arguments are the paths to the JSON file and the image directory.
register_coco_instances("plant_disease_train", {}, TRAIN_JSON_PATH, IMAGE_ROOT_DIR)
register_coco_instances("plant_disease_val", {}, VAL_JSON_PATH, IMAGE_ROOT_DIR)

# 4. Manually set the metadata to avoid common errors
# This ensures the model knows the names of your classes.
class_names = [cat['name'] for cat in categories]
MetadataCatalog.get("plant_disease_train").set(thing_classes=class_names)
MetadataCatalog.get("plant_disease_val").set(thing_classes=class_names)

print("\n‚úÖ Datasets registered successfully with Detectron2.")
print(f"Classes found: {class_names}")

# 5. (Optional) Verify the dataset by visualizing a few samples
print("\nVisualizing a few random samples from the training set to verify...")
plant_metadata = MetadataCatalog.get("plant_disease_train")
dataset_dicts = DatasetCatalog.get("plant_disease_train")

for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=plant_metadata, scale=1)
    out = visualizer.draw_dataset_dict(d)
    cv2_imshow(out.get_image()[:, :, ::-1])

"""#TRAINING FIRST 3000 ITERATIONS"""

#
# STEP 3: CONFIGURE AND TRAIN THE MASK R-CNN MODEL
#
# This script configures the model using transfer learning and starts the
# training loop.
#
# *** IMPORTANT FIX: The output directory is now set to your Google Drive
# to ensure all training progress is saved permanently. ***
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
import os

print("‚úÖ Libraries loaded. Starting model configuration.")

# --- Model Configuration ---

# 1. Create a standard Detectron2 config object
cfg = get_cfg()

# 2. Load a pre-trained model configuration from the "Model Zoo"
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Point to the pre-trained model weights
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)

# 4. Set our custom dataset names
cfg.DATASETS.TRAIN = ("plant_disease_train",)
cfg.DATASETS.TEST = ("plant_disease_val",)

# 5. Configure the Data Loader
cfg.DATALOADER.NUM_WORKERS = 2

# 6. Set Training Hyperparameters
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025
cfg.SOLVER.MAX_ITER = 3000
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

# 7. Set the number of classes
num_classes = len(MetadataCatalog.get("plant_disease_train").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes.")

# 8. Define the output directory
# --- ‚≠êÔ∏è CRUCIAL FIX ‚≠êÔ∏è ---
# We now save the output to your Google Drive to prevent data loss.
# A new folder will be created in your Drive to store the model checkpoints.
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/plant_disease_output"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Output will be saved permanently to: {cfg.OUTPUT_DIR}")


# --- Start Training ---
print("\nüöÄ Starting training... This will take some time.")
# Create the trainer instance with our custom configuration
trainer = DefaultTrainer(cfg)
# Load from the last checkpoint if it exists, otherwise start fresh
trainer.resume_or_load(resume=True) # Set to True to automatically resume
# Begin the training loop
trainer.train()

print(f"\nüéâ Training complete! Model saved in your Google Drive at '{DRIVE_OUTPUT_DIR}'.")

"""#RESULTS"""

#
# STEP 4: ADVANCED INFERENCE AND VISUALIZATION
#
# This script loads your trained model and creates a detailed, side-by-side
# comparison of the Ground Truth vs. Model Predictions for random images.
# It also determines a single, final classification based on the highest
# confidence score.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import cv2
import random
import os
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from google.colab.patches import cv2_imshow

print("‚úÖ Libraries loaded. Starting inference setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
# This must match the number of classes your model was trained on.
# Manually setting it is safest if you are running this in a new session.
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
# This is the most important step. We are loading the model you just trained.
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold for VISUALIZATION
# The visualizer will only draw masks with a score higher than this value.
# Our logic for finding the highest confidence will work regardless of this.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # A good starting point is 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Visualization ---
print("\nRunning inference on random validation images...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Define a smaller font for the visualizer
font_props = FontProperties(size=12)

# --- New Configuration: Number of images to visualize ---
NUM_IMAGES_TO_SHOW = 10

# Visualize predictions on the specified number of random images
for d in random.sample(val_dataset_dicts, NUM_IMAGES_TO_SHOW):
    im = cv2.imread(d["file_name"])

    # Make a prediction
    outputs = predictor(im)

    # --- Determine the single highest confidence prediction ---
    instances = outputs["instances"].to("cpu")
    final_prediction_title = "Model Prediction" # Default title

    if len(instances) > 0:
        scores = instances.scores
        pred_classes = instances.pred_classes
        max_score_index = torch.argmax(scores)
        highest_score = scores[max_score_index].item()
        predicted_class_index = pred_classes[max_score_index].item()
        predicted_class_name = val_metadata.thing_classes[predicted_class_index]

        final_prediction_title = (
            f"Final Classification: '{predicted_class_name}'\n"
            f"Confidence: {highest_score:.2%}"
        )
    else:
        final_prediction_title = "Model Prediction\n(No disease detected)"

    # --- Determine the ground truth class for the title ---
    ground_truth_title = "Ground Truth" # Default title
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        gt_class_name = val_metadata.thing_classes[gt_class_index]
        ground_truth_title = f"Ground Truth: '{gt_class_name}'"
    else:
        ground_truth_title = "Ground Truth\n(No annotations)"


    # --- Create a 1x2 plot for side-by-side comparison ---
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))
    fig.suptitle(f"File: {os.path.basename(d['file_name'])}", fontsize=20, y=0.93)

    # --- Panel 1: Ground Truth ---
    v_gt = Visualizer(im[:, :, ::-1],
                      metadata=val_metadata,
                      scale=0.8,
                      instance_mode=ColorMode.IMAGE_BW
    )
    gt_img = v_gt.draw_dataset_dict(d)
    ax1.imshow(gt_img.get_image()[:, :, ::-1])
    ax1.set_title(ground_truth_title, fontsize=22)
    ax1.axis("off")

    # --- Panel 2: Model Prediction ---
    v_pred = Visualizer(im[:, :, ::-1],
                        metadata=val_metadata,
                        scale=0.8
    )
    v_pred.metadata.font_size = 12
    pred_img = v_pred.draw_instance_predictions(instances)
    ax2.imshow(pred_img.get_image()[:, :, ::-1])
    ax2.set_title(final_prediction_title, fontsize=22)
    ax2.axis("off")

    plt.tight_layout()
    plt.show()



#
# STEP 4: QUANTITATIVE EVALUATION
#
# This script loads your trained model, runs it on the entire validation set,
# and calculates the final classification accuracy and a confusion matrix.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import os
import numpy as np
from tqdm import tqdm # For a nice progress bar
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("‚úÖ Libraries loaded. Starting evaluation setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold (optional, for consistency)
# This doesn't affect the "highest confidence" logic but is good practice.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Evaluation ---
print("\nRunning evaluation on the entire validation set...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Lists to store the ground truth and predicted labels
true_labels = []
predicted_labels = []

# Loop through the entire validation set with a progress bar
for d in tqdm(val_dataset_dicts, desc="Evaluating Images"):
    im = cv2.imread(d["file_name"])

    # --- Get Ground Truth Label ---
    # We only proceed if the image has a ground truth annotation
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        true_labels.append(gt_class_index)

        # --- Get Model Prediction ---
        outputs = predictor(im)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            # Apply the "highest confidence" rule
            scores = instances.scores
            pred_classes = instances.pred_classes
            max_score_index = torch.argmax(scores)
            predicted_class_index = pred_classes[max_score_index].item()
            predicted_labels.append(predicted_class_index)
        else:
            # If the model detects nothing, we assign a "no detection" class, e.g., -1
            predicted_labels.append(-1)

# --- Calculate and Print Accuracy ---
correct_predictions = 0
for true, pred in zip(true_labels, predicted_labels):
    if true == pred:
        correct_predictions += 1

total_images = len(true_labels)
accuracy = (correct_predictions / total_images) * 100

print("\n--- Evaluation Complete ---")
print(f"Total images evaluated: {total_images}")
print(f"Correct predictions: {correct_predictions}")
print(f"Accuracy: {accuracy:.2f}%")
print("---------------------------\n")


# --- Generate and Display Confusion Matrix ---
print("Generating confusion matrix...")
class_names = val_metadata.thing_classes

# The confusion matrix requires all labels to be within the range of class indices
# We'll map our -1 "no detection" class to an index outside the main classes for plotting
all_labels = sorted(list(set(true_labels + predicted_labels)))
display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]

cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

# Create a larger figure to fit the labels
fig, ax = plt.subplots(figsize=(15, 15))
disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
ax.set_title("Confusion Matrix", fontsize=20)
plt.tight_layout()
plt.show()

#
# Re-generates the classification report and confusion matrix if the
# 'true_labels' and 'predicted_labels' lists are still in memory.
#

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

try:
    # This will work if the previous evaluation cell was run in this session
    if 'true_labels' in locals() and 'predicted_labels' in locals():
        print("‚úÖ Found existing evaluation results in memory. Re-generating reports...")

        # --- Generate and Display Per-Class Metrics ---
        print("\n--- Per-Class Performance ---")
        class_names = val_metadata.thing_classes
        report_labels = sorted(list(set(true_labels + predicted_labels)))
        report_target_names = [class_names[i] if i >= 0 else "No Detection" for i in report_labels]
        report = classification_report(true_labels, predicted_labels, labels=report_labels, target_names=report_target_names)
        print(report)
        print("-----------------------------\n")

        # --- Generate and Display Confusion Matrix ---
        print("Generating confusion matrix...")
        all_labels = sorted(list(set(true_labels + predicted_labels)))
        display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]
        cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
        fig, ax = plt.subplots(figsize=(15, 15))
        disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
        ax.set_title("Confusion Matrix", fontsize=20)
        plt.tight_layout()
        plt.show()

    else:
        print("‚ùå Could not find previous results in memory. You may need to re-run the full evaluation script.")

except NameError:
    print("‚ùå A required variable (like 'val_metadata') was not found. You may need to re-run the full evaluation script.")

#
# STEP 5: VISUALIZE TRAINING LOSS FROM metrics.json
#
# This script parses the metrics.json file generated by Detectron2 during
# training and plots the key loss metrics to visualize the model's learning
# progress. This is the most accurate way to see the training curves.
#

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def parse_metrics_json(metrics_file_path):
    """
    Parses the Detectron2 metrics.json file into a pandas DataFrame.
    The file is expected to be in JSON Lines format (one JSON object per line).

    Args:
        metrics_file_path (str): The path to the metrics.json file.

    Returns:
        pandas.DataFrame: A DataFrame containing the parsed loss values.
    """
    records = []
    try:
        with open(metrics_file_path, 'r') as f:
            for line in f:
                # Each line is a separate JSON object
                records.append(json.loads(line))
    except FileNotFoundError:
        print(f"ERROR: Log file not found at '{metrics_file_path}'. Please check the path.")
        return None
    except json.JSONDecodeError as e:
        print(f"ERROR: Could not parse the JSON file. It may be corrupted. Error: {e}")
        return None

    if not records:
        print("ERROR: No data was parsed. The metrics file might be empty.")
        return None

    # Convert records to a DataFrame
    df = pd.DataFrame(records)

    # Filter out rows that don't contain loss information (e.g., evaluation metrics)
    df = df[df['total_loss'].notna()].reset_index(drop=True)

    # Combine the RPN losses for a cleaner plot
    if 'loss_rpn_cls' in df.columns and 'loss_rpn_loc' in df.columns:
        df['loss_rpn'] = df['loss_rpn_cls'] + df['loss_rpn_loc']

    return df

def plot_loss_curves(df, smoothing_window=20):
    """
    Generates and displays beautiful, smoothed plots of the training loss curves.

    Args:
        df (pandas.DataFrame): The DataFrame containing the parsed loss data.
        smoothing_window (int): The window size for the rolling average to smooth curves.
    """
    if df is None or df.empty:
        print("Cannot plot an empty DataFrame.")
        return

    # Set plot style for a professional look
    sns.set_theme(style="whitegrid")

    # Create a 2x2 subplot grid
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('Mask R-CNN Training Loss Analysis', fontsize=24, y=0.95)

    # --- Plot 1: Total Loss ---
    ax1 = axes[0, 0]
    sns.lineplot(data=df, x='iteration', y=df['total_loss'].rolling(smoothing_window).mean(), ax=ax1, color='black', label='Total Loss (Smoothed)')
    ax1.set_title('Overall Model Loss', fontsize=16)
    ax1.set_xlabel('Iteration', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend()
    ax1.set_ylim(bottom=0)

    # --- Plot 2: ROI Head Losses (Classification & Mask) ---
    ax2 = axes[0, 1]
    sns.lineplot(data=df, x='iteration', y=df['loss_cls'].rolling(smoothing_window).mean(), ax=ax2, label='Classification Loss (Smoothed)', color='royalblue')
    sns.lineplot(data=df, x='iteration', y=df['loss_mask'].rolling(smoothing_window).mean(), ax=ax2, label='Mask Loss (Smoothed)', color='darkorange')
    ax2.set_title('ROI Head Losses', fontsize=16)
    ax2.set_xlabel('Iteration', fontsize=12)
    ax2.set_ylabel('Loss', fontsize=12)
    ax2.legend()
    ax2.set_ylim(bottom=0)

    # --- Plot 3: Bounding Box Regression Loss ---
    ax3 = axes[1, 0]
    sns.lineplot(data=df, x='iteration', y=df['loss_box_reg'].rolling(smoothing_window).mean(), ax=ax3, label='BBox Regression Loss (Smoothed)', color='forestgreen')
    ax3.set_title('Bounding Box Regression Loss', fontsize=16)
    ax3.set_xlabel('Iteration', fontsize=12)
    ax3.set_ylabel('Loss', fontsize=12)
    ax3.legend()
    ax3.set_ylim(bottom=0)

    # --- Plot 4: Region Proposal Network (RPN) Loss ---
    ax4 = axes[1, 1]
    if 'loss_rpn' in df.columns:
        sns.lineplot(data=df, x='iteration', y=df['loss_rpn'].rolling(smoothing_window).mean(), ax=ax4, label='RPN Total Loss (Smoothed)', color='crimson')
        ax4.set_title('Region Proposal Network (RPN) Loss', fontsize=16)
        ax4.set_xlabel('Iteration', fontsize=12)
        ax4.set_ylabel('Loss', fontsize=12)
        ax4.legend()
        ax4.set_ylim(bottom=0)

    plt.tight_layout(rect=[0, 0, 1, 0.93])
    plt.show()

# --- Main Execution ---
if __name__ == '__main__':
    # --- CONFIGURATION ---
    # The path to your metrics.json file.
    METRICS_FILE_PATH = '/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output/metrics.json'
    # --- END CONFIGURATION ---

    # 1. Parse the metrics file into a DataFrame
    loss_df = parse_metrics_json(METRICS_FILE_PATH)

    # 2. Generate and display the plots
    if loss_df is not None:
        print(f"Successfully parsed {len(loss_df)} training steps from the log file.")
        plot_loss_curves(loss_df, smoothing_window=20)



"""#TRAINING TILL 6000 EPOCHS"""

#
# STEP 3: CONTINUE TRAINING THE MASK R-CNN MODEL
#
# This script is now configured to resume training from the last checkpoint
# and continue for more iterations with a learning rate scheduler to
# further improve performance.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
import os

print("‚úÖ Libraries loaded. Starting model configuration for continued training.")

# --- Model Configuration ---

# 1. Create a standard Detectron2 config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set our custom dataset names
cfg.DATASETS.TRAIN = ("plant_disease_train",)
cfg.DATASETS.TEST = ("plant_disease_val",)

# 4. Configure the Data Loader
cfg.DATALOADER.NUM_WORKERS = 2

# 5. Set Training Hyperparameters for Continued Training
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025 # Keep the initial learning rate the same

# --- ‚≠êÔ∏è KEY CHANGES FOR CONTINUED TRAINING ‚≠êÔ∏è ---
# Increase the total number of iterations
cfg.SOLVER.MAX_ITER = 6000
# Add a learning rate scheduler to fine-tune the model
# This will reduce the learning rate at iterations 4000 and 5000
cfg.SOLVER.STEPS = (4000, 5000)
cfg.SOLVER.GAMMA = 0.1 # The factor to reduce the LR by (e.g., 0.1 = 10x smaller)
# --- ‚≠êÔ∏è END OF KEY CHANGES ‚≠êÔ∏è ---

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

# 6. Set the number of classes
num_classes = len(MetadataCatalog.get("plant_disease_train").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes.")

# 7. Define the output directory (must be the same as before)
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"Output will be saved permanently to: {cfg.OUTPUT_DIR}")


# --- Start Continued Training ---
print("\nüöÄ Resuming training... This will take some time.")
# Create the trainer instance with our custom configuration
trainer = DefaultTrainer(cfg)
# Set resume=True to load the last checkpoint and continue training
trainer.resume_or_load(resume=True)
# Begin the training loop
trainer.train()

print(f"\nüéâ Training complete! Model updated in your Google Drive at '{DRIVE_OUTPUT_DIR}'.")

#
# STEP 4: ADVANCED INFERENCE AND VISUALIZATION
#
# This script loads your trained model and creates a detailed, side-by-side
# comparison of the Ground Truth vs. Model Predictions for random images.
# It also determines a single, final classification based on the highest
# confidence score.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import cv2
import random
import os
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from google.colab.patches import cv2_imshow

print("‚úÖ Libraries loaded. Starting inference setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
# This must match the number of classes your model was trained on.
# Manually setting it is safest if you are running this in a new session.
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
# This is the most important step. We are loading the model you just trained.
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold for VISUALIZATION
# The visualizer will only draw masks with a score higher than this value.
# Our logic for finding the highest confidence will work regardless of this.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # A good starting point is 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Visualization ---
print("\nRunning inference on random validation images...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Define a smaller font for the visualizer
font_props = FontProperties(size=12)

# --- New Configuration: Number of images to visualize ---
NUM_IMAGES_TO_SHOW = 10

# Visualize predictions on the specified number of random images
for d in random.sample(val_dataset_dicts, NUM_IMAGES_TO_SHOW):
    im = cv2.imread(d["file_name"])

    # Make a prediction
    outputs = predictor(im)

    # --- Determine the single highest confidence prediction ---
    instances = outputs["instances"].to("cpu")
    final_prediction_title = "Model Prediction" # Default title

    if len(instances) > 0:
        scores = instances.scores
        pred_classes = instances.pred_classes
        max_score_index = torch.argmax(scores)
        highest_score = scores[max_score_index].item()
        predicted_class_index = pred_classes[max_score_index].item()
        predicted_class_name = val_metadata.thing_classes[predicted_class_index]

        final_prediction_title = (
            f"Final Classification: '{predicted_class_name}'\n"
            f"Confidence: {highest_score:.2%}"
        )
    else:
        final_prediction_title = "Model Prediction\n(No disease detected)"

    # --- Determine the ground truth class for the title ---
    ground_truth_title = "Ground Truth" # Default title
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        gt_class_name = val_metadata.thing_classes[gt_class_index]
        ground_truth_title = f"Ground Truth: '{gt_class_name}'"
    else:
        ground_truth_title = "Ground Truth\n(No annotations)"


    # --- Create a 1x2 plot for side-by-side comparison ---
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))
    fig.suptitle(f"File: {os.path.basename(d['file_name'])}", fontsize=20, y=0.93)

    # --- Panel 1: Ground Truth ---
    v_gt = Visualizer(im[:, :, ::-1],
                      metadata=val_metadata,
                      scale=0.8,
                      instance_mode=ColorMode.IMAGE_BW
    )
    gt_img = v_gt.draw_dataset_dict(d)
    ax1.imshow(gt_img.get_image()[:, :, ::-1])
    ax1.set_title(ground_truth_title, fontsize=22)
    ax1.axis("off")

    # --- Panel 2: Model Prediction ---
    v_pred = Visualizer(im[:, :, ::-1],
                        metadata=val_metadata,
                        scale=0.8
    )
    v_pred.metadata.font_size = 12
    pred_img = v_pred.draw_instance_predictions(instances)
    ax2.imshow(pred_img.get_image()[:, :, ::-1])
    ax2.set_title(final_prediction_title, fontsize=22)
    ax2.axis("off")

    plt.tight_layout()
    plt.show()

#
# STEP 4: QUANTITATIVE EVALUATION
#
# This script loads your trained model, runs it on the entire validation set,
# and calculates the final classification accuracy and a confusion matrix.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import os
import numpy as np
from tqdm import tqdm # For a nice progress bar
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("‚úÖ Libraries loaded. Starting evaluation setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold (optional, for consistency)
# This doesn't affect the "highest confidence" logic but is good practice.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Evaluation ---
print("\nRunning evaluation on the entire validation set...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Lists to store the ground truth and predicted labels
true_labels = []
predicted_labels = []

# Loop through the entire validation set with a progress bar
for d in tqdm(val_dataset_dicts, desc="Evaluating Images"):
    im = cv2.imread(d["file_name"])

    # --- Get Ground Truth Label ---
    # We only proceed if the image has a ground truth annotation
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        true_labels.append(gt_class_index)

        # --- Get Model Prediction ---
        outputs = predictor(im)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            # Apply the "highest confidence" rule
            scores = instances.scores
            pred_classes = instances.pred_classes
            max_score_index = torch.argmax(scores)
            predicted_class_index = pred_classes[max_score_index].item()
            predicted_labels.append(predicted_class_index)
        else:
            # If the model detects nothing, we assign a "no detection" class, e.g., -1
            predicted_labels.append(-1)

# --- Calculate and Print Accuracy ---
correct_predictions = 0
for true, pred in zip(true_labels, predicted_labels):
    if true == pred:
        correct_predictions += 1

total_images = len(true_labels)
accuracy = (correct_predictions / total_images) * 100

print("\n--- Evaluation Complete ---")
print(f"Total images evaluated: {total_images}")
print(f"Correct predictions: {correct_predictions}")
print(f"Accuracy: {accuracy:.2f}%")
print("---------------------------\n")


# --- Generate and Display Confusion Matrix ---
print("Generating confusion matrix...")
class_names = val_metadata.thing_classes

# The confusion matrix requires all labels to be within the range of class indices
# We'll map our -1 "no detection" class to an index outside the main classes for plotting
all_labels = sorted(list(set(true_labels + predicted_labels)))
display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]

cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

# Create a larger figure to fit the labels
fig, ax = plt.subplots(figsize=(15, 15))
disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
ax.set_title("Confusion Matrix", fontsize=20)
plt.tight_layout()
plt.show()
#
# Re-generates the classification report and confusion matrix if the
# 'true_labels' and 'predicted_labels' lists are still in memory.
#

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

try:
    # This will work if the previous evaluation cell was run in this session
    if 'true_labels' in locals() and 'predicted_labels' in locals():
        print("‚úÖ Found existing evaluation results in memory. Re-generating reports...")

        # --- Generate and Display Per-Class Metrics ---
        print("\n--- Per-Class Performance ---")
        class_names = val_metadata.thing_classes
        report_labels = sorted(list(set(true_labels + predicted_labels)))
        report_target_names = [class_names[i] if i >= 0 else "No Detection" for i in report_labels]
        report = classification_report(true_labels, predicted_labels, labels=report_labels, target_names=report_target_names)
        print(report)
        print("-----------------------------\n")

        # --- Generate and Display Confusion Matrix ---
        print("Generating confusion matrix...")
        all_labels = sorted(list(set(true_labels + predicted_labels)))
        display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]
        cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
        fig, ax = plt.subplots(figsize=(15, 15))
        disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
        ax.set_title("Confusion Matrix", fontsize=20)
        plt.tight_layout()
        # plt.show()

    else:
        print("‚ùå Could not find previous results in memory. You may need to re-run the full evaluation script.")

except NameError:
    print("‚ùå A required variable (like 'val_metadata') was not found. You may need to re-run the full evaluation script.")

#
# STEP 5: VISUALIZE TRAINING LOSS FROM metrics.json
#
# This script parses the metrics.json file generated by Detectron2 during
# training and plots the key loss metrics to visualize the model's learning
# progress. This is the most accurate way to see the training curves.
#

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def parse_metrics_json(metrics_file_path):
    """
    Parses the Detectron2 metrics.json file into a pandas DataFrame.
    The file is expected to be in JSON Lines format (one JSON object per line).

    Args:
        metrics_file_path (str): The path to the metrics.json file.

    Returns:
        pandas.DataFrame: A DataFrame containing the parsed loss values.
    """
    records = []
    try:
        with open(metrics_file_path, 'r') as f:
            for line in f:
                # Each line is a separate JSON object
                records.append(json.loads(line))
    except FileNotFoundError:
        print(f"ERROR: Log file not found at '{metrics_file_path}'. Please check the path.")
        return None
    except json.JSONDecodeError as e:
        print(f"ERROR: Could not parse the JSON file. It may be corrupted. Error: {e}")
        return None

    if not records:
        print("ERROR: No data was parsed. The metrics file might be empty.")
        return None

    # Convert records to a DataFrame
    df = pd.DataFrame(records)

    # Filter out rows that don't contain loss information (e.g., evaluation metrics)
    df = df[df['total_loss'].notna()].reset_index(drop=True)

    # Combine the RPN losses for a cleaner plot
    if 'loss_rpn_cls' in df.columns and 'loss_rpn_loc' in df.columns:
        df['loss_rpn'] = df['loss_rpn_cls'] + df['loss_rpn_loc']

    return df

def plot_loss_curves(df, smoothing_window=20):
    """
    Generates and displays beautiful, smoothed plots of the training loss curves.

    Args:
        df (pandas.DataFrame): The DataFrame containing the parsed loss data.
        smoothing_window (int): The window size for the rolling average to smooth curves.
    """
    if df is None or df.empty:
        print("Cannot plot an empty DataFrame.")
        return

    # Set plot style for a professional look
    sns.set_theme(style="whitegrid")

    # Create a 2x2 subplot grid
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('Mask R-CNN Training Loss Analysis', fontsize=24, y=0.95)

    # --- Plot 1: Total Loss ---
    ax1 = axes[0, 0]
    sns.lineplot(data=df, x='iteration', y=df['total_loss'].rolling(smoothing_window).mean(), ax=ax1, color='black', label='Total Loss (Smoothed)')
    ax1.set_title('Overall Model Loss', fontsize=16)
    ax1.set_xlabel('Iteration', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend()
    ax1.set_ylim(bottom=0)

    # --- Plot 2: ROI Head Losses (Classification & Mask) ---
    ax2 = axes[0, 1]
    sns.lineplot(data=df, x='iteration', y=df['loss_cls'].rolling(smoothing_window).mean(), ax=ax2, label='Classification Loss (Smoothed)', color='royalblue')
    sns.lineplot(data=df, x='iteration', y=df['loss_mask'].rolling(smoothing_window).mean(), ax=ax2, label='Mask Loss (Smoothed)', color='darkorange')
    ax2.set_title('ROI Head Losses', fontsize=16)
    ax2.set_xlabel('Iteration', fontsize=12)
    ax2.set_ylabel('Loss', fontsize=12)
    ax2.legend()
    ax2.set_ylim(bottom=0)

    # --- Plot 3: Bounding Box Regression Loss ---
    ax3 = axes[1, 0]
    sns.lineplot(data=df, x='iteration', y=df['loss_box_reg'].rolling(smoothing_window).mean(), ax=ax3, label='BBox Regression Loss (Smoothed)', color='forestgreen')
    ax3.set_title('Bounding Box Regression Loss', fontsize=16)
    ax3.set_xlabel('Iteration', fontsize=12)
    ax3.set_ylabel('Loss', fontsize=12)
    ax3.legend()
    ax3.set_ylim(bottom=0)

    # --- Plot 4: Region Proposal Network (RPN) Loss ---
    ax4 = axes[1, 1]
    if 'loss_rpn' in df.columns:
        sns.lineplot(data=df, x='iteration', y=df['loss_rpn'].rolling(smoothing_window).mean(), ax=ax4, label='RPN Total Loss (Smoothed)', color='crimson')
        ax4.set_title('Region Proposal Network (RPN) Loss', fontsize=16)
        ax4.set_xlabel('Iteration', fontsize=12)
        ax4.set_ylabel('Loss', fontsize=12)
        ax4.legend()
        ax4.set_ylim(bottom=0)

    plt.tight_layout(rect=[0, 0, 1, 0.93])
    plt.show()

# --- Main Execution ---
if __name__ == '__main__':
    # --- CONFIGURATION ---
    # The path to your metrics.json file.
    METRICS_FILE_PATH = '/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output/metrics.json'
    # --- END CONFIGURATION ---

    # 1. Parse the metrics file into a DataFrame
    loss_df = parse_metrics_json(METRICS_FILE_PATH)

    # 2. Generate and display the plots
    if loss_df is not None:
        print(f"Successfully parsed {len(loss_df)} training steps from the log file.")
        plot_loss_curves(loss_df, smoothing_window=20)

"""# 8000 epochs

"""

#
# STEP 3: CONTINUE TRAINING THE MASK R-CNN MODEL (EXTENDED)
#
# This script is now configured to resume training from the last checkpoint
# and continue for 2000 more iterations, for a new total of 8000.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
import os

print("‚úÖ Libraries loaded. Starting model configuration for continued training.")

# --- Model Configuration ---

# 1. Create a standard Detectron2 config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set our custom dataset names
cfg.DATASETS.TRAIN = ("plant_disease_train",)
cfg.DATASETS.TEST = ("plant_disease_val",)

# 4. Configure the Data Loader
cfg.DATALOADER.NUM_WORKERS = 2

# 5. Set Training Hyperparameters for Continued Training
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025 # Keep the initial learning rate the same

# --- ‚≠êÔ∏è KEY CHANGES FOR EXTENDED TRAINING ‚≠êÔ∏è ---
# Increase the total number of iterations by 2000 (6000 -> 8000)
cfg.SOLVER.MAX_ITER = 8000
# Add a new step to the learning rate scheduler to fine-tune the model further
# This will reduce the learning rate at iterations 4000, 5000, and now 7000
cfg.SOLVER.STEPS = (4000, 5000, 7000)
cfg.SOLVER.GAMMA = 0.1 # The factor to reduce the LR by
# --- ‚≠êÔ∏è END OF KEY CHANGES ‚≠êÔ∏è ---

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

# 6. Set the number of classes
num_classes = len(MetadataCatalog.get("plant_disease_train").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes.")

# 7. Define the output directory (must be the same as before)
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"Output will be saved permanently to: {cfg.OUTPUT_DIR}")


# --- Start Continued Training ---
print("\nüöÄ Resuming training for 2000 more iterations... This will take some time.")
# Create the trainer instance with our custom configuration
trainer = DefaultTrainer(cfg)
# Set resume=True to load the last checkpoint and continue training to the new MAX_ITER
trainer.resume_or_load(resume=True)
# Begin the training loop
trainer.train()

print(f"\nüéâ Extended training complete! Model updated in your Google Drive at '{DRIVE_OUTPUT_DIR}'.")

#
# STEP 4: QUANTITATIVE EVALUATION
#
# This script loads your trained model, runs it on the entire validation set,
# and calculates the final classification accuracy and a confusion matrix.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import os
import numpy as np
from tqdm import tqdm # For a nice progress bar
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("‚úÖ Libraries loaded. Starting evaluation setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold (optional, for consistency)
# This doesn't affect the "highest confidence" logic but is good practice.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Evaluation ---
print("\nRunning evaluation on the entire validation set...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Lists to store the ground truth and predicted labels
true_labels = []
predicted_labels = []

# Loop through the entire validation set with a progress bar
for d in tqdm(val_dataset_dicts, desc="Evaluating Images"):
    im = cv2.imread(d["file_name"])

    # --- Get Ground Truth Label ---
    # We only proceed if the image has a ground truth annotation
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        true_labels.append(gt_class_index)

        # --- Get Model Prediction ---
        outputs = predictor(im)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            # Apply the "highest confidence" rule
            scores = instances.scores
            pred_classes = instances.pred_classes
            max_score_index = torch.argmax(scores)
            predicted_class_index = pred_classes[max_score_index].item()
            predicted_labels.append(predicted_class_index)
        else:
            # If the model detects nothing, we assign a "no detection" class, e.g., -1
            predicted_labels.append(-1)

# --- Calculate and Print Accuracy ---
correct_predictions = 0
for true, pred in zip(true_labels, predicted_labels):
    if true == pred:
        correct_predictions += 1

total_images = len(true_labels)
accuracy = (correct_predictions / total_images) * 100

print("\n--- Evaluation Complete ---")
print(f"Total images evaluated: {total_images}")
print(f"Correct predictions: {correct_predictions}")
print(f"Accuracy: {accuracy:.2f}%")
print("---------------------------\n")


# --- Generate and Display Confusion Matrix ---
print("Generating confusion matrix...")
class_names = val_metadata.thing_classes

# The confusion matrix requires all labels to be within the range of class indices
# We'll map our -1 "no detection" class to an index outside the main classes for plotting
all_labels = sorted(list(set(true_labels + predicted_labels)))
display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]

cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

# Create a larger figure to fit the labels
fig, ax = plt.subplots(figsize=(15, 15))
disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
ax.set_title("Confusion Matrix", fontsize=20)
plt.tight_layout()
plt.show()
#
# Re-generates the classification report and confusion matrix if the
# 'true_labels' and 'predicted_labels' lists are still in memory.
#

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

try:
    # This will work if the previous evaluation cell was run in this session
    if 'true_labels' in locals() and 'predicted_labels' in locals():
        print("‚úÖ Found existing evaluation results in memory. Re-generating reports...")

        # --- Generate and Display Per-Class Metrics ---
        print("\n--- Per-Class Performance ---")
        class_names = val_metadata.thing_classes
        report_labels = sorted(list(set(true_labels + predicted_labels)))
        report_target_names = [class_names[i] if i >= 0 else "No Detection" for i in report_labels]
        report = classification_report(true_labels, predicted_labels, labels=report_labels, target_names=report_target_names)
        print(report)
        print("-----------------------------\n")

        # --- Generate and Display Confusion Matrix ---
        print("Generating confusion matrix...")
        all_labels = sorted(list(set(true_labels + predicted_labels)))
        display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]
        cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
        fig, ax = plt.subplots(figsize=(15, 15))
        disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
        ax.set_title("Confusion Matrix", fontsize=20)
        plt.tight_layout()
        # plt.show()

    else:
        print("‚ùå Could not find previous results in memory. You may need to re-run the full evaluation script.")

except NameError:
    print("‚ùå A required variable (like 'val_metadata') was not found. You may need to re-run the full evaluation script.")

"""# Augmentation

"""

import json
import os
import cv2
import albumentations as A
import numpy as np
from tqdm import tqdm
from pycocotools import mask as mask_utils

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CONFIGURATION
# --------------------------------------------------------------------------

# --- Paths ---
# The main root directory containing all data
IMAGE_ROOT_DIR = "/content/drive/MyDrive/Dataset_crop_disease"
# The original COCO training annotation file
COCO_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_train.json")
# The name of the subfolder where new augmented images will be saved
RELATIVE_AUG_DIR = "train_augmented_masks"
# The full path to that new folder
FULL_AUG_DIR = os.path.join(IMAGE_ROOT_DIR, RELATIVE_AUG_DIR)
# Name for the new, updated COCO annotation file
NEW_COCO_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_train_augmented_masks.json")

# --- Augmentation Settings ---
TARGET_CLASS_NAME = "Potato__Healthy"
AUGMENTATIONS_PER_IMAGE = 5

os.makedirs(FULL_AUG_DIR, exist_ok=True)
print(f"Augmented images will be saved to: {FULL_AUG_DIR}")

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 2: DEFINE THE AUGMENTATION PIPELINE (No Changes Here)
# --------------------------------------------------------------------------
transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=30, p=0.8, border_mode=cv2.BORDER_CONSTANT, value=0),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),
    A.GaussNoise(p=0.3)
], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 3: LOAD DATA AND RUN AUGMENTATION (With Path Fix)
# --------------------------------------------------------------------------

with open(COCO_JSON_PATH, 'r') as f:
    coco_data = json.load(f)

cat_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}
target_cat_id = {v: k for k, v in cat_id_to_name.items()}[TARGET_CLASS_NAME]
image_ids_to_augment = {ann['image_id'] for ann in coco_data['annotations'] if ann['category_id'] == target_cat_id}
print(f"Found {len(image_ids_to_augment)} images to augment.")

new_images = []
new_annotations = []
new_image_id = max([img['id'] for img in coco_data['images']]) + 1
new_ann_id = max([ann['id'] for ann in coco_data['annotations']]) + 1

for image_info in tqdm(coco_data['images'], desc="Augmenting Images"):
    if image_info['id'] in image_ids_to_augment:
        # Load image using the full path
        image_path = os.path.join(IMAGE_ROOT_DIR, image_info['file_name'])
        image = cv2.imread(image_path)
        # ... (rest of the mask and annotation loading is the same)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]
        annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_info['id']]
        masks = []
        for ann in annotations:
            rle = mask_utils.frPyObjects(ann['segmentation'], h, w)
            binary_mask = mask_utils.decode(rle)
            if binary_mask.ndim > 2:
                binary_mask = np.sum(binary_mask, axis=2)
            masks.append(binary_mask.astype(np.uint8))
        bboxes = [ann['bbox'] for ann in annotations]
        category_ids = [ann['category_id'] for ann in annotations]

        for i in range(AUGMENTATIONS_PER_IMAGE):
            augmented = transform(image=image, masks=masks, bboxes=bboxes, category_ids=category_ids)
            aug_image = augmented['image']
            aug_masks = augmented['masks']

            # ‚≠êÔ∏è THE FIX: Create a filename that includes the relative path ‚≠êÔ∏è
            base_filename = os.path.basename(image_info['file_name'])
            new_simple_filename = f"{os.path.splitext(base_filename)[0]}_aug_mask_{i}.jpg"
            new_relative_path = os.path.join(RELATIVE_AUG_DIR, new_simple_filename)

            # Save the new image to the full path
            save_path = os.path.join(FULL_AUG_DIR, new_simple_filename)
            cv2.imwrite(save_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))

            # Store the new image entry using the RELATIVE path
            new_image_entry = {"id": new_image_id, "file_name": new_relative_path, "width": aug_image.shape[1], "height": aug_image.shape[0]}
            new_images.append(new_image_entry)

            # ... (rest of the annotation processing is the same)
            for mask, cat_id in zip(aug_masks, augmented['category_ids']):
                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if len(contours) > 0:
                    segmentation = []
                    for contour in contours:
                        if contour.size >= 6:
                            segmentation.append(contour.flatten().tolist())
                    if not segmentation: continue
                    x, y, new_w, new_h = cv2.boundingRect(contours[0])
                    new_ann_entry = {
                        "id": new_ann_id, "image_id": new_image_id, "category_id": cat_id,
                        "bbox": [x, y, new_w, new_h], "area": int(cv2.contourArea(contours[0])),
                        "segmentation": segmentation, "iscrowd": 0
                    }
                    new_annotations.append(new_ann_entry)
                    new_ann_id += 1
            new_image_id += 1

coco_data['images'].extend(new_images)
coco_data['annotations'].extend(new_annotations)

with open(NEW_COCO_JSON_PATH, 'w') as f:
    json.dump(coco_data, f)

print(f"\n‚úÖ Augmentation complete! New annotation file saved to: {NEW_COCO_JSON_PATH}")

# --- Imports ---
import json
import os
import cv2
import random
import matplotlib.pyplot as plt

# Import Detectron2 utilities
from detectron2.utils.visualizer import Visualizer
from detectron2.data import Metadata
from detectron2.structures import BoxMode

print("‚úÖ Libraries loaded.")

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CONFIGURATION
# --------------------------------------------------------------------------

# --- Paths ---
AUG_IMAGE_DIR = "/content/drive/MyDrive/Dataset_crop_disease/train_augmented"
NEW_COCO_JSON_PATH = "/content/drive/MyDrive/Dataset_crop_disease/coco_train_augmented_masks.json"

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 2: LOAD DATA AND VISUALIZE IN A GRID
# --------------------------------------------------------------------------

# Load the new, augmented COCO JSON file
with open(NEW_COCO_JSON_PATH, 'r') as f:
    coco_data = json.load(f)

# --- Create a temporary metadata object for visualization ---
class_names = [cat['name'] for cat in coco_data['categories']]
temp_metadata = Metadata()
temp_metadata.set(thing_classes=class_names)
print(f"Loaded {len(class_names)} classes.")


# --- Select random augmented images to display ---
augmented_images = [img for img in coco_data['images'] if '_aug_' in img['file_name']]

if len(augmented_images) < 6:
    print("‚ùå Not enough augmented images found to create a full grid. Please generate more.")
else:
    # --- Setup the subplot grid ---
    num_images_to_show = 9
    cols = 3
    rows = 3
    fig, axes = plt.subplots(rows, cols, figsize=(24, 16))
    fig.suptitle("Sample of Augmented Images and Annotations", fontsize=24)

    # Flatten the axes array for easy iteration
    axes = axes.flatten()

    # --- Loop to populate the grid ---
    for i, ax in enumerate(axes):
        # Pick one random augmented image
        random_image_info = random.choice(augmented_images)
        image_id = random_image_info['id']
        image_filename = random_image_info['file_name']

        # Load the image
        image_path = os.path.join(AUG_IMAGE_DIR, image_filename)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Find all annotations for this specific image
        annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]

        # Use Detectron2's Visualizer to draw the boxes
        visualizer = Visualizer(image, metadata=temp_metadata, scale=1.0)

        vis_annotations = []
        for ann in annotations:
            vis_annotations.append({
                "bbox": ann['bbox'],
                "bbox_mode": BoxMode.XYWH_ABS,
                "category_id": int(ann['category_id']),
            })

        output_image = visualizer.draw_dataset_dict({"annotations": vis_annotations})

        # --- Display the image in the correct subplot cell ---
        ax.imshow(output_image.get_image())
        ax.set_title(os.path.basename(image_filename), fontsize=12)
        ax.axis('off')

    # --- Display the entire grid at the end ---
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle
    plt.show()

import os
from detectron2.data.datasets import register_coco_instances
from detectron2.data import DatasetCatalog, MetadataCatalog

print("‚öôÔ∏è Starting dataset registration...")

# --- 1. Define Paths ---
# The main root directory where all image subfolders are located
IMAGE_ROOT_DIR = "/content/drive/MyDrive/Dataset_crop_disease"

# The path to your new, comprehensive training annotation file
AUGMENTED_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_train_augmented_masks.json")

# The path to your validation annotation file
VAL_JSON_PATH = os.path.join(IMAGE_ROOT_DIR, "coco_val.json")


# --- 2. Register the Augmented Training Set ---
try:
    register_coco_instances(
        name="plant_disease_train_augmented",
        metadata={},
        json_file=AUGMENTED_JSON_PATH,
        image_root=IMAGE_ROOT_DIR
    )
    print("‚úÖ Training dataset 'plant_disease_train_augmented' registered.")
except AssertionError:
    print("üí° Training dataset was already registered.")


# --- 3. Register the Validation Set ---
try:
    register_coco_instances(
        name="plant_disease_val",
        metadata={},
        json_file=VAL_JSON_PATH,
        image_root=IMAGE_ROOT_DIR
    )
    print("‚úÖ Validation dataset 'plant_disease_val' registered.")
except AssertionError:
    print("üí° Validation dataset was already registered.")


# --- 4. Verification Step ---
print("\n--- Verifying registered datasets ---")
try:
    # Check training set
    train_dicts = DatasetCatalog.get("plant_disease_train_augmented")
    print(f"   - Found {len(train_dicts)} images in 'plant_disease_train_augmented'.")

    # Check validation set
    val_dicts = DatasetCatalog.get("plant_disease_val")
    print(f"   - Found {len(val_dicts)} images in 'plant_disease_val'.")

    # Set metadata for classes (important for training)
    train_metadata = MetadataCatalog.get("plant_disease_train_augmented")
    val_metadata = MetadataCatalog.get("plant_disease_val")
    val_metadata.set(thing_classes=train_metadata.thing_classes)
    print("\n‚úÖ Registration and verification complete.")

except KeyError as e:
    print(f"‚ùå Error during verification. A dataset could not be found: {e}")

from detectron2.data import DatasetCatalog

# List of the dataset names you want to check
dataset_names_to_check = [
    "plant_disease_train_augmented",
    "plant_disease_val"
]

print("--- Checking Registered Datasets ---")

for name in dataset_names_to_check:
    try:
        # 'DatasetCatalog.get' loads the dataset's list of dictionaries
        dataset_dicts = DatasetCatalog.get(name)

        # The length of this list is the number of images
        num_images = len(dataset_dicts)

        print(f"‚úÖ Dataset '{name}' found with {num_images} images.")

    except KeyError:
        print(f"‚ùå Dataset '{name}' is not registered or could not be found.")

print("------------------------------------")

#
# FINAL SCRIPT: TRAINING WITH LESS FREQUENT EVALUATION
#
# This script trains the model from scratch and runs the validation
# check every 2000 iterations instead of 500.
#

# --- Core Imports ---
import torch, os, numpy as np, datetime, detectron2
from detectron2.utils.logger import setup_logger
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, build_detection_test_loader
from detectron2.engine import DefaultTrainer
from detectron2.engine.hooks import HookBase
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
setup_logger()

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CUSTOM CODE
# --------------------------------------------------------------------------

class CustomCOCOEvaluator(COCOEvaluator):
    """Applies an in-memory patch for the missing 'info' key in the JSON."""
    def __init__(self, dataset_name, output_dir=None):
        super().__init__(dataset_name, output_dir=output_dir)
        if "info" not in self._coco_api.dataset:
            print("--- INFO: 'info' key not found in JSON. Applying in-memory patch. ---")
            self._coco_api.dataset['info'] = {"description": "Patched In-Memory"}

class BestCheckpointer(HookBase):
    def __init__(self, eval_period: int, eval_loader, checkpointer):
        self._period = eval_period
        self._eval_loader = eval_loader
        self.checkpointer = checkpointer
        self._evaluator = None
        self._best_ap = -1.0
        self._best_iter = -1

    def before_train(self):
        self._evaluator = CustomCOCOEvaluator(self.trainer.cfg.DATASETS.TEST[0], output_dir=self.trainer.cfg.OUTPUT_DIR)

    def after_step(self):
        next_iter = self.trainer.iter + 1
        if next_iter % self._period == 0:
            eval_result = inference_on_dataset(self.trainer.model, self._eval_loader, self._evaluator)
            current_ap = eval_result["bbox"]["AP"]
            print(f"\n--- Iteration {next_iter}: Validation AP: {current_ap:.4f} ---\n")

            if current_ap > self._best_ap:
                self._best_ap = current_ap
                self._best_iter = self.trainer.iter
                self.checkpointer.save("model_best")
                print(f"üéâ New best model saved at iteration {self._best_iter} with AP: {self._best_ap:.4f}\n")

class CustomTrainer(DefaultTrainer):
    """A custom trainer that uses our BestCheckpointer hook."""
    def build_hooks(self):
        hooks = super().build_hooks()
        # ‚≠êÔ∏è CHANGE: Set how often to run evaluation. Increased from 500. ‚≠êÔ∏è
        eval_period = 2000
        eval_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])
        best_checkpointer_hook = BestCheckpointer(eval_period, eval_loader, self.checkpointer)
        hooks.insert(-1, best_checkpointer_hook)
        return hooks

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 2: MODEL CONFIGURATION
# --------------------------------------------------------------------------

cfg = get_cfg()
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)

cfg.DATASETS.TRAIN = ("plant_disease_train_augmented",)
cfg.DATASETS.TEST = ("plant_disease_val",)

cfg.DATALOADER.NUM_WORKERS = 2
cfg.DATALOADER.SAMPLER_TRAIN = "RepeatFactorTrainingSampler"
cfg.DATALOADER.REPEAT_THRESHOLD = 0.02

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025
cfg.SOLVER.MAX_ITER = 8000
cfg.SOLVER.STEPS = (4000, 6000)
cfg.SOLVER.GAMMA = 0.1

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
num_classes = len(MetadataCatalog.get("plant_disease_train_augmented").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes.")

DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output_v3"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"Output for this new run will be saved to: {cfg.OUTPUT_DIR}")

# --------------------------------------------------------------------------
# üöÄ SECTION 3: START TRAINING
# --------------------------------------------------------------------------
print("\nüöÄ Starting new training run...")

trainer = CustomTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

print(f"\n‚úÖ New training run complete! The best model was saved as 'model_best.pth' in '{cfg.OUTPUT_DIR}'.")

import json

# Path to your validation annotation file
VAL_JSON_PATH = "/content/drive/MyDrive/Dataset_crop_disease/coco_val.json"

print(f"Attempting to fix 'area' key in: {VAL_JSON_PATH}")

try:
    with open(VAL_JSON_PATH, 'r') as f:
        coco_data = json.load(f)

    annotations_fixed = 0
    if 'annotations' in coco_data:
        for ann in coco_data['annotations']:
            if 'area' not in ann or ann['area'] == 0:
                bbox = ann['bbox']
                ann['area'] = bbox[2] * bbox[3] # area = width * height
                annotations_fixed += 1

    if annotations_fixed > 0:
        print(f"Found and fixed {annotations_fixed} annotations missing the 'area' key.")
        with open(VAL_JSON_PATH, 'w') as f:
            json.dump(coco_data, f)
        print("‚úÖ Successfully added the 'area' key and saved the file!")
    else:
        print("‚úÖ No annotations needed fixing.")

except Exception as e:
    print(f"An error occurred: {e}")

"""# Balanced dataset"""

#
# FINAL SCRIPT: STARTING A NEW TRAINING RUN FROM SCRATCH
#
# This script includes:
# 1. Configuration to start a fresh run (resume=False).
# 2. A new output directory to keep results clean.
# 3. Class imbalance sampler and in-memory JSON patch.
#

# --- Core Imports ---
import torch
import os
import numpy as np
import datetime

# --- Detectron2 Imports ---
import detectron2
from detectron2.utils.logger import setup_logger
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, build_detection_test_loader
from detectron2.engine import DefaultTrainer
from detectron2.engine.hooks import HookBase
from detectron2.evaluation import COCOEvaluator, inference_on_dataset

# Initialize Detectron2's logger
setup_logger()
print("‚úÖ All libraries loaded successfully.")

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CUSTOM CODE
# --------------------------------------------------------------------------

class CustomCOCOEvaluator(COCOEvaluator):
    """Applies an in-memory patch for the missing 'info' key in the JSON."""
    def __init__(self, dataset_name, output_dir=None):
        super().__init__(dataset_name, output_dir=output_dir)
        if "info" not in self._coco_api.dataset:
            print("--- INFO: 'info' key not found in JSON. Applying in-memory patch. ---")
            self._coco_api.dataset['info'] = {"description": "Patched In-Memory"}

class BestCheckpointer(HookBase):
    """Saves the best model checkpoint based on validation AP."""
    def __init__(self, eval_period: int, eval_loader, checkpointer):
        self._period = eval_period
        self._eval_loader = eval_loader
        self.checkpointer = checkpointer
        self._evaluator = None
        self._best_ap = -1.0
        self._best_iter = -1

    def before_train(self):
        self._evaluator = CustomCOCOEvaluator(self.trainer.cfg.DATASETS.TEST[0], output_dir=self.trainer.cfg.OUTPUT_DIR)

    def after_step(self):
        next_iter = self.trainer.iter + 1
        if next_iter % self._period == 0:
            eval_result = inference_on_dataset(self.trainer.model, self._eval_loader, self._evaluator)
            current_ap = eval_result["bbox"]["AP"]
            print(f"\n--- Iteration {next_iter}: Validation AP: {current_ap:.4f} ---\n")

            if current_ap > self._best_ap:
                self._best_ap = current_ap
                self._best_iter = self.trainer.iter
                self.checkpointer.save("model_best")
                print(f"üéâ New best model saved at iteration {self._best_iter} with AP: {self._best_ap:.4f}\n")

class CustomTrainer(DefaultTrainer):
    """A custom trainer that uses our BestCheckpointer hook."""
    def build_hooks(self):
        hooks = super().build_hooks()
        eval_period = 500
        eval_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])
        best_checkpointer_hook = BestCheckpointer(eval_period, eval_loader, self.checkpointer)
        hooks.insert(-1, best_checkpointer_hook)
        return hooks

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 2: MODEL CONFIGURATION
# --------------------------------------------------------------------------

cfg = get_cfg()
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)

cfg.DATASETS.TRAIN = ("plant_disease_train",)
cfg.DATASETS.TEST = ("plant_disease_val",)
cfg.DATALOADER.NUM_WORKERS = 2

cfg.DATALOADER.SAMPLER_TRAIN = "RepeatFactorTrainingSampler"
cfg.DATALOADER.REPEAT_THRESHOLD = 0.02

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025
cfg.SOLVER.MAX_ITER = 8000
cfg.SOLVER.STEPS = (4000, 6000)
cfg.SOLVER.GAMMA = 0.1

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
num_classes = len(MetadataCatalog.get("plant_disease_train").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes.")

# ‚≠êÔ∏è CHANGE 1: (Recommended) Set a new output directory for the new run ‚≠êÔ∏è
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output_v2"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"Output for this new run will be saved to: {cfg.OUTPUT_DIR}")

# --------------------------------------------------------------------------
# üöÄ SECTION 3: START TRAINING
# --------------------------------------------------------------------------
print("\nüöÄ Starting a new training run from scratch...")

trainer = CustomTrainer(cfg)

# ‚≠êÔ∏è CHANGE 2: Set resume=False to start a new run from iteration 0 ‚≠êÔ∏è
trainer.resume_or_load(resume=False)

trainer.train()

print(f"\n‚úÖ New training run complete! The best model was saved as 'model_best.pth' in your output directory.")

#
# STEP 4: QUANTITATIVE EVALUATION
#
# This script loads your trained model, runs it on the entire validation set,
# and calculates the final classification accuracy and a confusion matrix.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import os
import numpy as np
from tqdm import tqdm # For a nice progress bar
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("‚úÖ Libraries loaded. Starting evaluation setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output_v2"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold (optional, for consistency)
# This doesn't affect the "highest confidence" logic but is good practice.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Evaluation ---
print("\nRunning evaluation on the entire validation set...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Lists to store the ground truth and predicted labels
true_labels = []
predicted_labels = []

# Loop through the entire validation set with a progress bar
for d in tqdm(val_dataset_dicts, desc="Evaluating Images"):
    im = cv2.imread(d["file_name"])

    # --- Get Ground Truth Label ---
    # We only proceed if the image has a ground truth annotation
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        true_labels.append(gt_class_index)

        # --- Get Model Prediction ---
        outputs = predictor(im)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            # Apply the "highest confidence" rule
            scores = instances.scores
            pred_classes = instances.pred_classes
            max_score_index = torch.argmax(scores)
            predicted_class_index = pred_classes[max_score_index].item()
            predicted_labels.append(predicted_class_index)
        else:
            # If the model detects nothing, we assign a "no detection" class, e.g., -1
            predicted_labels.append(-1)

# --- Calculate and Print Accuracy ---
correct_predictions = 0
for true, pred in zip(true_labels, predicted_labels):
    if true == pred:
        correct_predictions += 1

total_images = len(true_labels)
accuracy = (correct_predictions / total_images) * 100

print("\n--- Evaluation Complete ---")
print(f"Total images evaluated: {total_images}")
print(f"Correct predictions: {correct_predictions}")
print(f"Accuracy: {accuracy:.2f}%")
print("---------------------------\n")


# --- Generate and Display Confusion Matrix ---
print("Generating confusion matrix...")
class_names = val_metadata.thing_classes

# The confusion matrix requires all labels to be within the range of class indices
# We'll map our -1 "no detection" class to an index outside the main classes for plotting
all_labels = sorted(list(set(true_labels + predicted_labels)))
display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]

cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

# Create a larger figure to fit the labels
fig, ax = plt.subplots(figsize=(15, 15))
disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
ax.set_title("Confusion Matrix", fontsize=20)
plt.tight_layout()
plt.show()
#
# Re-generates the classification report and confusion matrix if the
# 'true_labels' and 'predicted_labels' lists are still in memory.
#

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

try:
    # This will work if the previous evaluation cell was run in this session
    if 'true_labels' in locals() and 'predicted_labels' in locals():
        print("‚úÖ Found existing evaluation results in memory. Re-generating reports...")

        # --- Generate and Display Per-Class Metrics ---
        print("\n--- Per-Class Performance ---")
        class_names = val_metadata.thing_classes
        report_labels = sorted(list(set(true_labels + predicted_labels)))
        report_target_names = [class_names[i] if i >= 0 else "No Detection" for i in report_labels]
        report = classification_report(true_labels, predicted_labels, labels=report_labels, target_names=report_target_names)
        print(report)
        print("-----------------------------\n")

        # --- Generate and Display Confusion Matrix ---
        print("Generating confusion matrix...")
        all_labels = sorted(list(set(true_labels + predicted_labels)))
        display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]
        cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
        fig, ax = plt.subplots(figsize=(15, 15))
        disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
        ax.set_title("Confusion Matrix", fontsize=20)
        plt.tight_layout()
        # plt.show()

    else:
        print("‚ùå Could not find previous results in memory. You may need to re-run the full evaluation script.")

except NameError:
    print("‚ùå A required variable (like 'val_metadata') was not found. You may need to re-run the full evaluation script.")

#
# STEP 5: VISUAL ANALYSIS
#
# This script loads your trained model and draws its predictions on a sample
# of validation images to help you qualitatively analyze its performance.
#

# --- Imports ---
import torch
import cv2
import os
import random
import matplotlib.pyplot as plt

import detectron2
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.utils.visualizer import Visualizer
from detectron2 import model_zoo

print("‚úÖ Libraries loaded. Starting visualization setup.")

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CONFIGURATION AND MODEL LOADING
# --------------------------------------------------------------------------

cfg = get_cfg()
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# Get the number of classes from the registered dataset metadata
val_metadata = MetadataCatalog.get("plant_disease_val")
num_classes = len(val_metadata.thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# --- Using the paths you specified ---
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output_v2"
# Point to the final model weights as requested
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")
print(f"\n‚úÖ Loading weights from: {cfg.MODEL.WEIGHTS}")

# Set the confidence threshold for prediction
# Detections with a score lower than this will not be shown
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
predictor = DefaultPredictor(cfg)


# --------------------------------------------------------------------------
# üñºÔ∏è SECTION 2: VISUALIZE PREDICTIONS
# --------------------------------------------------------------------------
print("\nGenerating visualizations for random validation images...")

# Get the dictionary of the validation dataset
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")

# Randomly select a few images to visualize
num_images_to_show = 5
for d in random.sample(val_dataset_dicts, num_images_to_show):
    # Load the image using OpenCV
    img_path = d["file_name"]
    im = cv2.imread(img_path)

    # Run the image through the predictor
    outputs = predictor(im)
    instances = outputs["instances"].to("cpu")

    # --- Create the Visualizer ---
    # We will draw the model's predictions on the image
    v = Visualizer(
        im[:, :, ::-1], # Convert BGR to RGB for matplotlib
        metadata=val_metadata,
        scale=0.8
    )

    # Draw the instance predictions (boxes, masks, labels)
    vis_output = v.draw_instance_predictions(instances)

    # --- Display the Image ---
    print(f"--- Displaying prediction for: {os.path.basename(img_path)} ---")
    plt.figure(figsize=(14, 10))
    plt.imshow(vis_output.get_image())
    plt.axis('off')
    plt.show()

#
# STEP 6: MEAN AVERAGE PRECISION (mAP) EVALUATION (FULLY CORRECTED)
#
# This script uses an improved CustomCOCOEvaluator to apply in-memory patches
# for both the 'info' and 'area' keys, resolving all known data format errors.
#

# --- Imports ---
import torch
import os
import datetime
import detectron2
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.data import build_detection_test_loader, MetadataCatalog
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2 import model_zoo

print("‚úÖ Libraries loaded. Starting mAP evaluation.")

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 1: CUSTOM EVALUATOR (IMPROVED)
# --------------------------------------------------------------------------

class CustomCOCOEvaluator(COCOEvaluator):
    """
    A custom evaluator that applies in-memory patches for missing
    'info' and 'area' keys in the COCO JSON file.
    """
    def __init__(self, dataset_name, output_dir=None):
        # Initialize the parent class, which loads the JSON file
        super().__init__(dataset_name, output_dir=output_dir)

        # --- FIX #1: Patch the top-level 'info' key ---
        if "info" not in self._coco_api.dataset:
            print("\n--- INFO: 'info' key not found. Applying in-memory patch. ---")
            self._coco_api.dataset['info'] = {
                "description": "Patched In-Memory",
            }

        # --- FIX #2: Patch the 'area' key in individual annotations ---
        annotations = self._coco_api.dataset.get("annotations", [])
        annotations_patched = 0
        for ann in annotations:
            if 'area' not in ann:
                # Calculate area from bounding box [x, y, width, height]
                bbox = ann['bbox']
                ann['area'] = bbox[2] * bbox[3]
                annotations_patched += 1

        if annotations_patched > 0:
            print(f"--- INFO: Patched {annotations_patched} annotations missing the 'area' key. ---\n")


# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 2: CONFIGURATION AND MODEL LOADING
# --------------------------------------------------------------------------

cfg = get_cfg()
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# Get the number of classes from your registered dataset
num_classes = len(MetadataCatalog.get("plant_disease_val").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# Point to your best-trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output_v2"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_best.pth")
print(f"\n‚úÖ Loading weights for evaluation from: {cfg.MODEL.WEIGHTS}")

# Set the confidence threshold
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
predictor = DefaultPredictor(cfg)

# --------------------------------------------------------------------------
# ‚öôÔ∏è SECTION 3: RUN EVALUATION
# --------------------------------------------------------------------------
print("\nRunning COCO-style evaluation on the validation set...")

# Use our improved CustomCOCOEvaluator
evaluator = CustomCOCOEvaluator("plant_disease_val", output_dir=cfg.OUTPUT_DIR)
val_loader = build_detection_test_loader(cfg, "plant_disease_val")

# Run the inference
results = inference_on_dataset(predictor.model, val_loader, evaluator)

print("\n--- Evaluation Complete ---")
print("Mean Average Precision (mAP) Results:")
print(results)
print("---------------------------\n")

import json

# --- 1. DEFINE THE PATH TO YOUR JSON FILE ---
json_file_path = "/content/drive/MyDrive/Dataset_crop_disease/coco_val.json"

print(f"Attempting to fix 'area' key in: {json_file_path}")

try:
    # --- 2. LOAD THE JSON DATA ---
    with open(json_file_path, 'r') as f:
        coco_data = json.load(f)

    # --- 3. ITERATE AND FIX ANNOTATIONS ---
    annotations_fixed = 0
    # Ensure the 'annotations' key exists
    if 'annotations' in coco_data:
        for ann in coco_data['annotations']:
            # Check if the 'area' key is missing
            if 'area' not in ann:
                # Get the bounding box: [x, y, width, height]
                bbox = ann['bbox']
                # Calculate area as width * height
                area = bbox[2] * bbox[3]
                # Add the 'area' key
                ann['area'] = area
                annotations_fixed += 1

    if annotations_fixed > 0:
        print(f"Found and fixed {annotations_fixed} annotations missing the 'area' key.")
        # --- 4. SAVE THE MODIFIED DATA ---
        with open(json_file_path, 'w') as f:
            json.dump(coco_data, f, indent=4)
        print("‚úÖ Successfully added the 'area' key and saved the file!")
    else:
        print("‚úÖ No annotations were missing the 'area' key. No changes needed.")

except FileNotFoundError:
    print(f"‚ùå ERROR: The file was not found at {json_file_path}. Please check the path.")
except Exception as e:
    print(f"An error occurred: {e}")

#
# This script compresses the specified output folder into a single .zip file.
# It uses a robust method that changes the directory to prevent common path errors.
#

import os

# --- Configuration ---
# 1. The path to the folder you want to zip.
#    This should be the output directory from your training script.
FOLDER_TO_ZIP = "/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output"

# 2. The path and name for the new zip file you want to create.
#    This will save the zip file in the same parent directory.
OUTPUT_ZIP_PATH = "/content/drive/MyDrive/plant_disease_output.zip"
# --- End Configuration ---


print(f"Starting to zip the folder: {FOLDER_TO_ZIP}")

# --- Robust Zipping Logic ---
# Get the parent directory and the folder name to be zipped
parent_dir = os.path.dirname(FOLDER_TO_ZIP)
folder_name = os.path.basename(FOLDER_TO_ZIP)

# Change to the parent directory. This is the key to making zip work reliably.
# We store the original directory to change back to it later.
original_dir = os.getcwd()
try:
    os.chdir(parent_dir)
    print(f"Changed directory to: {parent_dir}")

    # Now, run the zip command on the folder name (not the full path)
    # The output path must be the full, absolute path.
    print(f"Zipping '{folder_name}' into '{OUTPUT_ZIP_PATH}'...")
    !zip -r -q "{OUTPUT_ZIP_PATH}" "{folder_name}"

finally:
    # Always change back to the original directory
    os.chdir(original_dir)
    print(f"Returned to original directory: {original_dir}")


# Check if the file was created successfully
if os.path.exists(OUTPUT_ZIP_PATH):
    print(f"\n‚úÖ Success! Zip file created at: {OUTPUT_ZIP_PATH}")
else:
    print(f"\n‚ùå Error: Failed to create the zip file. Please check that the source folder path is correct.")

#
# STEP 3: CONFIGURE AND TRAIN A MORE POWERFUL MODEL (ResNet-101)
#
# This script is now configured to use a larger and more powerful
# ResNet-101 backbone for potentially higher accuracy.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
import os

print("‚úÖ Libraries loaded. Starting model configuration.")

# --- Model Configuration ---

# 1. Create a standard Detectron2 config object
cfg = get_cfg()

# 2. --- ‚≠êÔ∏è KEY CHANGE: LOAD A MORE POWERFUL BACKBONE ‚≠êÔ∏è ---
# We are now using a Mask R-CNN with a ResNet-101 backbone.
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Point to the pre-trained model weights for the new backbone
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)
# --- ‚≠êÔ∏è END OF KEY CHANGE ‚≠êÔ∏è ---

# 4. Set our custom dataset names
cfg.DATASETS.TRAIN = ("plant_disease_train",)
cfg.DATASETS.TEST = ("plant_disease_val",)

# 5. Configure the Data Loader
cfg.DATALOADER.NUM_WORKERS = 2

# 6. Set Training Hyperparameters
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0025
# Resetting iterations for the new training run
cfg.SOLVER.MAX_ITER = 6000 # A good starting point for a larger model
cfg.SOLVER.STEPS = [] # Disable scheduler for now to see the baseline
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

# 7. Set the number of classes
num_classes = len(MetadataCatalog.get("plant_disease_train").thing_classes)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"\nModel configured for {num_classes} classes using a ResNet-101 backbone.")

# 8. Define a new output directory for this experiment
# This prevents overwriting your previous model's results.
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/plant_disease_output_resnet101"
cfg.OUTPUT_DIR = DRIVE_OUTPUT_DIR
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
print(f"Output will be saved permanently to: {cfg.OUTPUT_DIR}")


# --- Start Training ---
print("\nüöÄ Starting training for the new model... This will take longer than before.")
# Create the trainer instance with our custom configuration
trainer = DefaultTrainer(cfg)
# Start fresh since this is a new model architecture
trainer.resume_or_load(resume=True)
# Begin the training loop
trainer.train()

print(f"\nüéâ Training complete! New model saved in your Google Drive at '{DRIVE_OUTPUT_DIR}'.")

#
# STEP 4: ADVANCED INFERENCE AND VISUALIZATION
#
# This script loads your trained model and creates a detailed, side-by-side
# comparison of the Ground Truth vs. Model Predictions for random images.
# It also determines a single, final classification based on the highest
# confidence score.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import cv2
import random
import os
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from google.colab.patches import cv2_imshow

print("‚úÖ Libraries loaded. Starting inference setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
# This must match the number of classes your model was trained on.
# Manually setting it is safest if you are running this in a new session.
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
# This is the most important step. We are loading the model you just trained.
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/plant_disease_output_resnet101"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold for VISUALIZATION
# The visualizer will only draw masks with a score higher than this value.
# Our logic for finding the highest confidence will work regardless of this.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # A good starting point is 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Visualization ---
print("\nRunning inference on random validation images...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Define a smaller font for the visualizer
font_props = FontProperties(size=12)

# --- New Configuration: Number of images to visualize ---
NUM_IMAGES_TO_SHOW = 10

# Visualize predictions on the specified number of random images
for d in random.sample(val_dataset_dicts, NUM_IMAGES_TO_SHOW):
    im = cv2.imread(d["file_name"])

    # Make a prediction
    outputs = predictor(im)

    # --- Determine the single highest confidence prediction ---
    instances = outputs["instances"].to("cpu")
    final_prediction_title = "Model Prediction" # Default title

    if len(instances) > 0:
        scores = instances.scores
        pred_classes = instances.pred_classes
        max_score_index = torch.argmax(scores)
        highest_score = scores[max_score_index].item()
        predicted_class_index = pred_classes[max_score_index].item()
        predicted_class_name = val_metadata.thing_classes[predicted_class_index]

        final_prediction_title = (
            f"Final Classification: '{predicted_class_name}'\n"
            f"Confidence: {highest_score:.2%}"
        )
    else:
        final_prediction_title = "Model Prediction\n(No disease detected)"

    # --- Determine the ground truth class for the title ---
    ground_truth_title = "Ground Truth" # Default title
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        gt_class_name = val_metadata.thing_classes[gt_class_index]
        ground_truth_title = f"Ground Truth: '{gt_class_name}'"
    else:
        ground_truth_title = "Ground Truth\n(No annotations)"


    # --- Create a 1x2 plot for side-by-side comparison ---
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))
    fig.suptitle(f"File: {os.path.basename(d['file_name'])}", fontsize=20, y=0.93)

    # --- Panel 1: Ground Truth ---
    v_gt = Visualizer(im[:, :, ::-1],
                      metadata=val_metadata,
                      scale=0.8,
                      instance_mode=ColorMode.IMAGE_BW
    )
    gt_img = v_gt.draw_dataset_dict(d)
    ax1.imshow(gt_img.get_image()[:, :, ::-1])
    ax1.set_title(ground_truth_title, fontsize=22)
    ax1.axis("off")

    # --- Panel 2: Model Prediction ---
    v_pred = Visualizer(im[:, :, ::-1],
                        metadata=val_metadata,
                        scale=0.8
    )
    v_pred.metadata.font_size = 12
    pred_img = v_pred.draw_instance_predictions(instances)
    ax2.imshow(pred_img.get_image()[:, :, ::-1])
    ax2.set_title(final_prediction_title, fontsize=22)
    ax2.axis("off")

    plt.tight_layout()
    plt.show()

#
# STEP 4: QUANTITATIVE EVALUATION
#
# This script loads your trained model, runs it on the entire validation set,
# and calculates the final classification accuracy and a confusion matrix.
#

# --- Imports (ensure these are loaded if in a new session) ---
import torch, torchvision
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2 import model_zoo

import os
import numpy as np
from tqdm import tqdm # For a nice progress bar
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("‚úÖ Libraries loaded. Starting evaluation setup.")

# --- Configuration ---

# 1. Create a config object
cfg = get_cfg()

# 2. Load the same model configuration file
config_file = "COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(config_file))

# 3. Set the number of classes
num_classes = 10
cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes
print(f"Model configured for {num_classes} classes.")

# 4. Point to your trained model weights
DRIVE_OUTPUT_DIR = "/content/drive/MyDrive/plant_disease_output_resnet101"
cfg.MODEL.WEIGHTS = os.path.join(DRIVE_OUTPUT_DIR, "model_final.pth")

# 5. Set a confidence threshold (optional, for consistency)
# This doesn't affect the "highest confidence" logic but is good practice.
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

# 6. Create the predictor instance
predictor = DefaultPredictor(cfg)
print(f"‚úÖ Predictor loaded with weights from: {cfg.MODEL.WEIGHTS}")


# --- Run Evaluation ---
print("\nRunning evaluation on the entire validation set...")

# Get the dataset dictionaries and metadata for the validation set
val_dataset_dicts = DatasetCatalog.get("plant_disease_val")
val_metadata = MetadataCatalog.get("plant_disease_val")

# Lists to store the ground truth and predicted labels
true_labels = []
predicted_labels = []

# Loop through the entire validation set with a progress bar
for d in tqdm(val_dataset_dicts, desc="Evaluating Images"):
    im = cv2.imread(d["file_name"])

    # --- Get Ground Truth Label ---
    # We only proceed if the image has a ground truth annotation
    if d["annotations"]:
        # Assuming one class per image, get the class from the first annotation
        gt_class_index = d["annotations"][0]['category_id']
        true_labels.append(gt_class_index)

        # --- Get Model Prediction ---
        outputs = predictor(im)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            # Apply the "highest confidence" rule
            scores = instances.scores
            pred_classes = instances.pred_classes
            max_score_index = torch.argmax(scores)
            predicted_class_index = pred_classes[max_score_index].item()
            predicted_labels.append(predicted_class_index)
        else:
            # If the model detects nothing, we assign a "no detection" class, e.g., -1
            predicted_labels.append(-1)

# --- Calculate and Print Accuracy ---
correct_predictions = 0
for true, pred in zip(true_labels, predicted_labels):
    if true == pred:
        correct_predictions += 1

total_images = len(true_labels)
accuracy = (correct_predictions / total_images) * 100

print("\n--- Evaluation Complete ---")
print(f"Total images evaluated: {total_images}")
print(f"Correct predictions: {correct_predictions}")
print(f"Accuracy: {accuracy:.2f}%")
print("---------------------------\n")


# --- Generate and Display Confusion Matrix ---
print("Generating confusion matrix...")
class_names = val_metadata.thing_classes

# The confusion matrix requires all labels to be within the range of class indices
# We'll map our -1 "no detection" class to an index outside the main classes for plotting
all_labels = sorted(list(set(true_labels + predicted_labels)))
display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]

cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

# Create a larger figure to fit the labels
fig, ax = plt.subplots(figsize=(15, 15))
disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
ax.set_title("Confusion Matrix", fontsize=20)
plt.tight_layout()
plt.show()
#
# Re-generates the classification report and confusion matrix if the
# 'true_labels' and 'predicted_labels' lists are still in memory.
#

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

try:
    # This will work if the previous evaluation cell was run in this session
    if 'true_labels' in locals() and 'predicted_labels' in locals():
        print("‚úÖ Found existing evaluation results in memory. Re-generating reports...")

        # --- Generate and Display Per-Class Metrics ---
        print("\n--- Per-Class Performance ---")
        class_names = val_metadata.thing_classes
        report_labels = sorted(list(set(true_labels + predicted_labels)))
        report_target_names = [class_names[i] if i >= 0 else "No Detection" for i in report_labels]
        report = classification_report(true_labels, predicted_labels, labels=report_labels, target_names=report_target_names)
        print(report)
        print("-----------------------------\n")

        # --- Generate and Display Confusion Matrix ---
        print("Generating confusion matrix...")
        all_labels = sorted(list(set(true_labels + predicted_labels)))
        display_labels = [class_names[i] if i >= 0 else "No Detection" for i in all_labels]
        cm = confusion_matrix(true_labels, predicted_labels, labels=all_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
        fig, ax = plt.subplots(figsize=(15, 15))
        disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
        ax.set_title("Confusion Matrix", fontsize=20)
        plt.tight_layout()
        # plt.show()

    else:
        print("‚ùå Could not find previous results in memory. You may need to re-run the full evaluation script.")

except NameError:
    print("‚ùå A required variable (like 'val_metadata') was not found. You may need to re-run the full evaluation script.")

#
# STEP 5: VISUALIZE TRAINING LOSS FROM metrics.json
#
# This script parses the metrics.json file generated by Detectron2 during
# training and plots the key loss metrics to visualize the model's learning
# progress. This is the most accurate way to see the training curves.
#

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def parse_metrics_json(metrics_file_path):
    """
    Parses the Detectron2 metrics.json file into a pandas DataFrame.
    The file is expected to be in JSON Lines format (one JSON object per line).

    Args:
        metrics_file_path (str): The path to the metrics.json file.

    Returns:
        pandas.DataFrame: A DataFrame containing the parsed loss values.
    """
    records = []
    try:
        with open(metrics_file_path, 'r') as f:
            for line in f:
                # Each line is a separate JSON object
                records.append(json.loads(line))
    except FileNotFoundError:
        print(f"ERROR: Log file not found at '{metrics_file_path}'. Please check the path.")
        return None
    except json.JSONDecodeError as e:
        print(f"ERROR: Could not parse the JSON file. It may be corrupted. Error: {e}")
        return None

    if not records:
        print("ERROR: No data was parsed. The metrics file might be empty.")
        return None

    # Convert records to a DataFrame
    df = pd.DataFrame(records)

    # Filter out rows that don't contain loss information (e.g., evaluation metrics)
    df = df[df['total_loss'].notna()].reset_index(drop=True)

    # Combine the RPN losses for a cleaner plot
    if 'loss_rpn_cls' in df.columns and 'loss_rpn_loc' in df.columns:
        df['loss_rpn'] = df['loss_rpn_cls'] + df['loss_rpn_loc']

    return df

def plot_loss_curves(df, smoothing_window=20):
    """
    Generates and displays beautiful, smoothed plots of the training loss curves.

    Args:
        df (pandas.DataFrame): The DataFrame containing the parsed loss data.
        smoothing_window (int): The window size for the rolling average to smooth curves.
    """
    if df is None or df.empty:
        print("Cannot plot an empty DataFrame.")
        return

    # Set plot style for a professional look
    sns.set_theme(style="whitegrid")

    # Create a 2x2 subplot grid
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('Mask R-CNN Training Loss Analysis', fontsize=24, y=0.95)

    # --- Plot 1: Total Loss ---
    ax1 = axes[0, 0]
    sns.lineplot(data=df, x='iteration', y=df['total_loss'].rolling(smoothing_window).mean(), ax=ax1, color='black', label='Total Loss (Smoothed)')
    ax1.set_title('Overall Model Loss', fontsize=16)
    ax1.set_xlabel('Iteration', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend()
    ax1.set_ylim(bottom=0)

    # --- Plot 2: ROI Head Losses (Classification & Mask) ---
    ax2 = axes[0, 1]
    sns.lineplot(data=df, x='iteration', y=df['loss_cls'].rolling(smoothing_window).mean(), ax=ax2, label='Classification Loss (Smoothed)', color='royalblue')
    sns.lineplot(data=df, x='iteration', y=df['loss_mask'].rolling(smoothing_window).mean(), ax=ax2, label='Mask Loss (Smoothed)', color='darkorange')
    ax2.set_title('ROI Head Losses', fontsize=16)
    ax2.set_xlabel('Iteration', fontsize=12)
    ax2.set_ylabel('Loss', fontsize=12)
    ax2.legend()
    ax2.set_ylim(bottom=0)

    # --- Plot 3: Bounding Box Regression Loss ---
    ax3 = axes[1, 0]
    sns.lineplot(data=df, x='iteration', y=df['loss_box_reg'].rolling(smoothing_window).mean(), ax=ax3, label='BBox Regression Loss (Smoothed)', color='forestgreen')
    ax3.set_title('Bounding Box Regression Loss', fontsize=16)
    ax3.set_xlabel('Iteration', fontsize=12)
    ax3.set_ylabel('Loss', fontsize=12)
    ax3.legend()
    ax3.set_ylim(bottom=0)

    # --- Plot 4: Region Proposal Network (RPN) Loss ---
    ax4 = axes[1, 1]
    if 'loss_rpn' in df.columns:
        sns.lineplot(data=df, x='iteration', y=df['loss_rpn'].rolling(smoothing_window).mean(), ax=ax4, label='RPN Total Loss (Smoothed)', color='crimson')
        ax4.set_title('Region Proposal Network (RPN) Loss', fontsize=16)
        ax4.set_xlabel('Iteration', fontsize=12)
        ax4.set_ylabel('Loss', fontsize=12)
        ax4.legend()
        ax4.set_ylim(bottom=0)

    plt.tight_layout(rect=[0, 0, 1, 0.93])
    plt.show()

# --- Main Execution ---
if __name__ == '__main__':
    # --- CONFIGURATION ---
    # The path to your metrics.json file.
    METRICS_FILE_PATH = '/content/drive/MyDrive/Dataset_crop_disease/plant_disease_output/metrics.json'
    # --- END CONFIGURATION ---

    # 1. Parse the metrics file into a DataFrame
    loss_df = parse_metrics_json(METRICS_FILE_PATH)

    # 2. Generate and display the plots
    if loss_df is not None:
        print(f"Successfully parsed {len(loss_df)} training steps from the log file.")
        plot_loss_curves(loss_df, smoothing_window=20)